

source .venv/bin/activate
sbatch slurm_sae
sbatch slurm_sentiment
sbatch slurm_labelling - gemma needs this version pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3, https://huggingface.co/google/gemma-3-27b-it/discussions/36

sbatch slurm_opt_prompt
sbatch slurm_llm_opt_test

sbatch slurm_gen_comp
sbatch slurm_sentiment
sbatch slurm_labelling
sbatch slurm_mech
sbatch slurm_refusal


squeue -u ana42

sintr -A MLMI-ana42-SL2-CPU -p icelake -N1 -n1 -t 1:0:0 --qos=INTR
sintr -A MLMI-ana42-SL2-GPU -p ampere --gres=gpu:1 -N1 -n1 -t 1:0:0 --qos=INTR
sintr -A MLMI-ana42-SL2-GPU -p ampere --gres=gpu:1 -N1 -n1 -t 0:20:0 --qos=INTR

sintr -A MLMI-ana42-SL2-GPU -p ampere --gres=gpu:2 -N1 -n1 -t 01:00:0 --qos=INTR

rm -rf ~/.cache/huggingface/hub/models--google--gemma-3-27b-it
rm -rf ~/.cache/huggingface/hub/models--google--gemma-2-2b


python3 z_data/generate_completions.py
python3 z_data/sentiment_counts.py
python3 z_data/judge_bias.py

/home/ana42/rds/hpc-work/sae_entities/notes.txt

toch gemma3 issue
https://huggingface.co/google/gemma-2-2b-it/discussions/57
for sentiment updgraded to  torch==2.6.0 
pip install torch==2.6.0  --index-url https://download.pytorch.org/whl/cu124
transformers == 4.48.0


python -m utils.activation_cache --model_alias gemma-2-2b --tokens_to_cache bias --batch_size 128 --dataset bias
python -m utils.activation_cache --model_alias gemma-2-2b --tokens_to_cache random --batch_size 128 --dataset pile
python3 -m mech_interp.feature_analysis

------- IGNORE ----------
python -m utils.activation_cache --model_alias gemma-2-2b-it --tokens_to_cache bias --batch_size 128 --dataset bias
python -m utils.activation_cache --model_alias gemma-2-2b --tokens_to_cache model --batch_size 128 --dataset wikidata
-------------------------
python3 -m mech_interp.refusal_analysis
python3 -m mech_interp.refusal_analysis_bias_test
python3 -m mech_interp.steering_playground


Number of unknown entities: 532
tokenized_prompts torch.Size([1596, 33])
queries 153239
Number of known entities: 1607
Number of unknown entities: 532
tokenized_prompts torch.Size([1596, 32])
Loading steering latents for model: gemma-2-2b
Loading steering latents for model: gemma-2-2b
Loading steering latents for model: gemma-2-2b
Loading random latents
Traceback (most recent call last):
  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 112, in _get_module_details
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/steering_it.py", line 137, in <module>
    known_latent, unknown_latent, random_latents_known, random_latents_unknown = load_latents(model_alias, top_latents,
                                                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/mech_interp_utils.py", line 1675, in load_latents
    random_latents_known: List[Tuple[int, float, Tensor]] = load_steering_latents('movie', label='known', topk=kwargs['random_n_latents'],
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/mech_interp_utils.py", line 425, in load_steering_latents
    min_max_scores = json.load(open(f"./train_latents_layers_entities/absolute_difference/{model_alias.split('/')[-1]}/entity/sorted_scores_min_{label}.json"))
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './train_latents_layers_entities/absolute_difference/gemma-2-2b/entity/sorted_scores_min_known.json'




Loading steering latents for model: gemma-2-2b, Label: bias, Random: True
Warning: `sorted_scores_min_bias.json` not found at ./train_latents_layers_bias/absolute_difference/gemma-2-2b/bias/sorted_scores_min_bias.json. Cannot filter random latents by score.
Loading steering latents for model: gemma-2-2b, Label: unbias, Random: True
Warning: `sorted_scores_min_unbias.json` not found at ./train_latents_layers_bias/absolute_difference/gemma-2-2b/bias/sorted_scores_min_unbias.json. Cannot filter random latents by score.




Explaination of the layer evolution plot one entity.
When a latent is processed, its score list only ever has one value: [score_Race_ethnicity].
The np.min() of a list with only one number is just the number itself. The "minimum" calculation does nothing.
Therefore, your MaxMin value for each layer simply becomes the maximum score out of all latents for Race_ethnicity.
The Final Conclusion
Your plot is comparing:

Blue Line: The average of the Top 5 scores for Race_ethnicity.
Red Line: The single best score out of all latents for Race_ethnicity.
It is mathematically guaranteed that the single best score will almost always be higher than the average of the top 5 scores. Your plot is visually different because it is accurately representing this mathematical fact for a single-entity analysis.



TODO
in activation_cache.py - model_alias upate and data_path = f"z_data/test_prompt_final/test_{category}_completion_sentiment_judged_final.jsonl" # TO UPDATE AND MOVE TO laod_data.py
ABOVE created load_bias_queries(path_prefix: str = "z_data/test_prompt_final") in dataset/load_data.py, called in utils/activation_cache.py and 
mech_interp/feature_analysis_utils.py



bias_bias_Race_ethnicity
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis.py", line 146, in <module>
    scatter_plot_latent_separation_scores_experiment(model_alias, tokenizer, bias_type_update,
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis_utils.py", line 266, in scatter_plot_latent_separation_scores_experiment
    dataloader = get_dataloader(model_alias, tokens_to_cache, n_layers, d_model, f'bias_{entity_type}', batch_size=16)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis_utils.py", line 50, in get_dataloader
    shard_size = bias_shard_size_entities[bias_type]
                 ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^
KeyError: 'bias_Race_ethnicity'





Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.19it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [00:01<00:00, 583.06it/s]
Loading SAE for layer 1
Loading SAE for layer 2
Loading SAE for layer 3
Loading SAE for layer 4
Loading SAE for layer 5
Loading SAE for layer 6
Loading SAE for layer 7
Loading SAE for layer 8
Loading SAE for layer 9
Loading SAE for layer 10
Loading SAE for layer 11
Loading SAE for layer 12
Loading SAE for layer 13
Loading SAE for layer 14
Loading SAE for layer 15
Loading SAE for layer 16
Loading SAE for layer 17
Loading SAE for layer 18
Loading SAE for layer 19
Loading SAE for layer 20
Loading SAE for layer 21
Loading SAE for layer 22
Loading SAE for layer 23
Loading SAE for layer 24
Loading SAE for layer 25
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis.py", line 96, in <module>
    get_per_layer_latent_scores(
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis_utils.py", line 290, in get_per_layer_latent_scores
    dataloader = get_dataloader(
                 ^^^^^^^^^^^^^^^
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis_utils.py", line 87, in get_dataloader
    dataset = CachedDataset(
              ^^^^^^^^^^^^^^
  File "/rds/user/ana42/hpc-work/sae_entities/utils/activation_cache.py", line 444, in __init__
    acts = np.memmap(
           ^^^^^^^^^^
  File "/rds/user/ana42/hpc-work/sae_entities/.venv/lib64/python3.11/site-packages/numpy/core/memmap.py", line 268, in __new__
    mm = mmap.mmap(fid.fileno(), bytes, access=acc, offset=start)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: mmap length is greater than file size














notes
utils/activation_cache.py:
Main script for caching model activations
Takes prompts and saves the model's hidden states (activations) to disk
Handles different datasets:
Bias data (Race_ethnicity, Nationality, Religion, Gender_identity)
Pile dataset (general language data)
Wikidata (entity knowledge)
TriviaQA (question answering)
Saves activations in memory-mapped files for efficient loading
For bias data, also saves bias labels (1 for biased, 0 for unbiased)
mech_interp/feature_analysis.py:
Main analysis script that orchestrates the feature analysis
Computes latent scores for each layer
Generates scatter plots of latent separation scores
Identifies top general latents with Pile filtering
Creates layerwise latent score plots
Saves results in:
./plots/scatter_plots/ for scatter plots
./plots/layerwise_evolution/ for layerwise plots
./train_latents_layers_entities/ for JSON files
mech_interp/feature_analysis_utils.py:
Helper functions for feature analysis
get_dataloader: Loads cached activations from disk
get_acts_labels_dict_: Processes activations and labels
get_per_layer_latent_scores: Computes latent scores per layer
get_general_latents: Identifies general latents across categories
get_layerwise_latent_scores: Analyzes how latents evolve across layers
plot_layerwise_latent_scores: Creates visualizations of layerwise scores
The workflow is:
First run activation_cache.py to cache activations for both bias data and Pile dataset
Then run feature_analysis.py to analyze the cached activations
The analysis will help identify:
Which features in the model are most sensitive to bias
How these features evolve across layers
Which features are specific to bias vs. general language patterns
How well the model separates biased from unbiased content
Would you like me to explain any particular part in more detail?

test