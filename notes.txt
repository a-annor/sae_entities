

source .venv/bin/activate
sbatch slurm_sae
sbatch slurm_sentiment
sbatch slurm_labelling - gemma needs this version pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3, https://huggingface.co/google/gemma-3-27b-it/discussions/36

sbatch slurm_opt_prompt
sbatch slurm_llm_opt_test

sbatch slurm_gen_comp
sbatch slurm_sentiment
sbatch slurm_labelling
sbatch slurm_mech


squeue -u ana42

sintr -A MLMI-ana42-SL2-CPU -p icelake -N1 -n1 -t 1:0:0 --qos=INTR
sintr -A MLMI-ana42-SL2-GPU -p ampere --gres=gpu:1 -N1 -n1 -t 1:0:0 --qos=INTR
sintr -A MLMI-ana42-SL2-GPU -p ampere --gres=gpu:1 -N1 -n1 -t 0:20:0 --qos=INTR

sintr -A MLMI-ana42-SL2-GPU -p ampere --gres=gpu:2 -N1 -n1 -t 1:0:0 --qos=INTR

rm -rf ~/.cache/huggingface/hub/models--google--gemma-3-27b-it
rm -rf ~/.cache/huggingface/hub/models--google--gemma-2-2b


python3 z_my_data/generate_completions.py
python3 z_my_data/sentiment_counts.py
python3 z_my_data/judge_bias.py

/home/ana42/rds/hpc-work/sae_entities/notes.txt

toch gemma3 issue
https://huggingface.co/google/gemma-2-2b-it/discussions/57
for sentiment updgraded to  torch==2.6.0 
pip install torch==2.6.0  --index-url https://download.pytorch.org/whl/cu124
transformers == 4.48.0


python -m utils.activation_cache --model_alias gemma-2-2b --tokens_to_cache bias --batch_size 128 --dataset bias
python -m utils.activation_cache --model_alias gemma-2-2b --tokens_to_cache random --batch_size 128 --dataset pile

python3 -m mech_interp.feature_analysis

2984867096, 0.2792208194732666, 0.25974026322364807, 0.25, 0.25], 20: [0.350649356842041, 0.26298701763153076, 0.2564935088157654, 0.25, 0.2467532604932785], 21: [0.3636363744735718, 0.27272728085517883, 0.26298701763153076, 0.25, 0.25], 22: [0.29870131611824036, 0.26298701763153076, 0.25974026322364807, 0.24025976657867432, 0.23701298236846924], 23: [0.3181818127632141, 0.31168830394744873, 0.29870131611824036, 0.25, 0.25], 24: [0.25, 0.25, 0.25, 0.2142857164144516, 0.2077922224998474], 25: [0.26298704743385315, 0.25974026322364807, 0.25, 0.25, 0.23701298236846924]}), 'Nationality': defaultdict(<class 'list'>, {}), 'Religion': defaultdict(<class 'list'>, {}), 'Gender_identity': defaultdict(<class 'list'>, {})}}
plot minmax_layerwise_scores:  {'bias': {}, 'unbias': {}}
/rds/user/ana42/hpc-work/sae_entities/.venv/lib64/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning:

Mean of empty slice.

/rds/user/ana42/hpc-work/sae_entities/.venv/lib64/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning:

invalid value encountered in scalar divide

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis.py", line 171, in <module>
    plot_layerwise_latent_scores(model_alias, LAYERS_WITH_SAE, top_scores_layers,
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis_utils.py", line 457, in plot_layerwise_latent_scores
    min_scores = [min(top_scores_layers[known_label][entity_type][layer]) for layer in sae_layers]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis_utils.py", line 457, in <listcomp>
    min_scores = [min(top_scores_layers[known_label][entity_type][layer]) for layer in sae_layers]
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: min() arg is an empty sequence

TODO
in activation_cache.py - model_alias upate and data_path = f"z_my_data/test_prompt_final/test_{category}_completion_sentiment_judged_final.jsonl" # TO UPDATE AND MOVE TO laod_data.py
ABOVE created load_bias_queries(path_prefix: str = "z_my_data/test_prompt_final") in dataset/load_data.py, called in utils/activation_cache.py and 
mech_interp/feature_analysis_utils.py



bias_bias_Race_ethnicity
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis.py", line 146, in <module>
    scatter_plot_latent_separation_scores_experiment(model_alias, tokenizer, bias_type_update,
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis_utils.py", line 266, in scatter_plot_latent_separation_scores_experiment
    dataloader = get_dataloader(model_alias, tokens_to_cache, n_layers, d_model, f'bias_{entity_type}', batch_size=16)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis_utils.py", line 50, in get_dataloader
    shard_size = bias_shard_size_entities[bias_type]
                 ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^
KeyError: 'bias_Race_ethnicity'





Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.19it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [00:01<00:00, 583.06it/s]
Loading SAE for layer 1
Loading SAE for layer 2
Loading SAE for layer 3
Loading SAE for layer 4
Loading SAE for layer 5
Loading SAE for layer 6
Loading SAE for layer 7
Loading SAE for layer 8
Loading SAE for layer 9
Loading SAE for layer 10
Loading SAE for layer 11
Loading SAE for layer 12
Loading SAE for layer 13
Loading SAE for layer 14
Loading SAE for layer 15
Loading SAE for layer 16
Loading SAE for layer 17
Loading SAE for layer 18
Loading SAE for layer 19
Loading SAE for layer 20
Loading SAE for layer 21
Loading SAE for layer 22
Loading SAE for layer 23
Loading SAE for layer 24
Loading SAE for layer 25
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis.py", line 96, in <module>
    get_per_layer_latent_scores(
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis_utils.py", line 290, in get_per_layer_latent_scores
    dataloader = get_dataloader(
                 ^^^^^^^^^^^^^^^
  File "/rds/user/ana42/hpc-work/sae_entities/mech_interp/feature_analysis_utils.py", line 87, in get_dataloader
    dataset = CachedDataset(
              ^^^^^^^^^^^^^^
  File "/rds/user/ana42/hpc-work/sae_entities/utils/activation_cache.py", line 444, in __init__
    acts = np.memmap(
           ^^^^^^^^^^
  File "/rds/user/ana42/hpc-work/sae_entities/.venv/lib64/python3.11/site-packages/numpy/core/memmap.py", line 268, in __new__
    mm = mmap.mmap(fid.fileno(), bytes, access=acc, offset=start)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: mmap length is greater than file size














notes
utils/activation_cache.py:
Main script for caching model activations
Takes prompts and saves the model's hidden states (activations) to disk
Handles different datasets:
Bias data (Race_ethnicity, Nationality, Religion, Gender_identity)
Pile dataset (general language data)
Wikidata (entity knowledge)
TriviaQA (question answering)
Saves activations in memory-mapped files for efficient loading
For bias data, also saves bias labels (1 for biased, 0 for unbiased)
mech_interp/feature_analysis.py:
Main analysis script that orchestrates the feature analysis
Computes latent scores for each layer
Generates scatter plots of latent separation scores
Identifies top general latents with Pile filtering
Creates layerwise latent score plots
Saves results in:
./plots/scatter_plots/ for scatter plots
./plots/layerwise_evolution/ for layerwise plots
./train_latents_layers_entities/ for JSON files
mech_interp/feature_analysis_utils.py:
Helper functions for feature analysis
get_dataloader: Loads cached activations from disk
get_acts_labels_dict_: Processes activations and labels
get_per_layer_latent_scores: Computes latent scores per layer
get_general_latents: Identifies general latents across categories
get_layerwise_latent_scores: Analyzes how latents evolve across layers
plot_layerwise_latent_scores: Creates visualizations of layerwise scores
The workflow is:
First run activation_cache.py to cache activations for both bias data and Pile dataset
Then run feature_analysis.py to analyze the cached activations
The analysis will help identify:
Which features in the model are most sensitive to bias
How these features evolve across layers
Which features are specific to bias vs. general language patterns
How well the model separates biased from unbiased content
Would you like me to explain any particular part in more detail?

test