{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import einops\n",
    "from typing import List, Tuple, Literal, Union\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch import Tensor\n",
    "from colorama import Fore\n",
    "import textwrap\n",
    "import gc\n",
    "import copy\n",
    "import re\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import transformer_lens.utils as tl_utils\n",
    "from utils.sae_utils import load_sae\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.activation_cache import _get_activations_fixed_seq_len\n",
    "from utils.hf_models.model_factory import construct_model_base\n",
    "from utils.utils import model_alias_to_model_name\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hf_models.gemma_model import format_instruction_gemma_chat\n",
    "from utils.hf_models.llama3_model import format_instruction_llama3_chat\n",
    "\n",
    "model_alias = 'meta-llama/Llama-3.1-8B'\n",
    "\n",
    "if 'gemma' in model_alias.lower():  \n",
    "    format_instructions_chat_fn = partial(format_instruction_gemma_chat, output=None, system=None, include_trailing_whitespace=True)\n",
    "elif 'llama' in model_alias.lower():\n",
    "    format_instructions_chat_fn = partial(format_instruction_llama3_chat, output=None, system=None, include_trailing_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = model_alias_to_model_name[model_alias]\n",
    "model = HookedTransformer.from_pretrained_no_processing(model_path)\n",
    "tokenizer = model.tokenizer\n",
    "tokenizer.padding_side = 'left'\n",
    "model_alias = model_alias.replace('/', '_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mech_interp_utils import layer_sparisity_widths, model_alias_to_sae_repo_id\n",
    "\n",
    "def load_sae_(model_alias, layer, width='16k', repo_id=\"google/gemma-scope-2b-pt-res\"):\n",
    "    \"\"\"\n",
    "    Loads the sae for a given layer and width.\n",
    "    Calls the load_sae function with the correct parameters.\n",
    "    \"\"\"\n",
    "    sae_width = '16k'\n",
    "    # sae_sparsity = layer_sparisity_widths[layer-1][width]\n",
    "    # sae_id = f\"layer_{layer-1}/width_{width}/average_l0_{str(sae_sparsity)}\"\n",
    "    if model_alias == 'gemma-2b-it':\n",
    "        assert layer == 13, \"Layer 13 is the only layer for gemma-2b-it\"\n",
    "        sae_id = \"gemma-2b-it-res-jb\"\n",
    "        repo_id = model_alias_to_sae_repo_id[model_alias]\n",
    "    elif model_alias == 'meta-llama_Llama-3.1-8B':\n",
    "        sae_id = f\"l{layer-1}r_8x\"\n",
    "    elif model_alias == 'gemma-2-9b-it':\n",
    "        assert layer in [10, 21, 32], \"Layer 10, 21, 32 are the only layers for gemma-2-9b-it\"\n",
    "        sae_sparsity = layer_sparisity_widths[model_alias][layer-1][sae_width]\n",
    "        sae_id = f\"layer_{layer-1}/width_{sae_width}/average_l0_{str(sae_sparsity)}\"\n",
    "        repo_id = model_alias_to_sae_repo_id[model_alias]\n",
    "    else:\n",
    "        sae_id = f\"layer_{layer-1}/width_{sae_width}/average_l0_{str(layer_sparisity_widths[model_alias][layer-1][sae_width])}\"\n",
    "        repo_id = model_alias_to_sae_repo_id[model_alias]\n",
    "\n",
    "    sae = load_sae(repo_id, sae_id)\n",
    "    return sae\n",
    "\n",
    "def get_activations_hook_fn(activation, hook, layer, cache: Float[Tensor, \"pos layer d_model\"], n_samples, positions: List[int]):\n",
    "    cache += activation[:, positions, :]\n",
    "    return activation\n",
    "\n",
    "def get_cache(\n",
    "    model: HookedTransformer,\n",
    "    tokenizer,\n",
    "    prompts: List[str],\n",
    "    act_name='resid_pre',\n",
    "    cache_layers=None,\n",
    "    positions=None,\n",
    "    batch_size=32\n",
    ") -> Tensor:\n",
    "\n",
    "    n_layers = model.cfg.n_layers\n",
    "    d_model = model.cfg.d_model\n",
    "    d_mlp = model.cfg.d_mlp\n",
    "    model_dtype = model.cfg.dtype\n",
    "\n",
    "    if act_name == 'post':\n",
    "        act_dim = d_mlp\n",
    "    else:\n",
    "        act_dim = d_model\n",
    "\n",
    "    if cache_layers is None:\n",
    "        cache_layers = list(range(0, n_layers))\n",
    "\n",
    "    toks = tokenizer(prompts, padding=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    if positions is None:\n",
    "        positions = list(range(0, toks.size(-1)))\n",
    "\n",
    "    activations: Float[Tensor, \"batch_size layer seq_len d_model\"] = torch.zeros((len(prompts), len(cache_layers), len(positions), act_dim), dtype=model_dtype, device='cuda')\n",
    "\n",
    "    for batch_idx in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch_end = min(len(prompts), batch_idx+batch_size)\n",
    "        batch_toks = toks[batch_idx:batch_end]\n",
    "\n",
    "        fwd_hooks = [\n",
    "            (\n",
    "                tl_utils.get_act_name(act_name, layer),\n",
    "                partial(get_activations_hook_fn, layer=layer, cache=activations[batch_idx:batch_end, layer_idx, :, :], n_samples=len(prompts), positions=positions)\n",
    "            )\n",
    "            for layer_idx, layer in enumerate(cache_layers)\n",
    "        ]\n",
    "\n",
    "        model.run_with_hooks(batch_toks, fwd_hooks=fwd_hooks)\n",
    "\n",
    "    return toks, activations\n",
    "\n",
    "@dataclass\n",
    "class PerTokenLatentActivations:\n",
    "    prompt: str\n",
    "    token_ids: List[int]\n",
    "    token_strs: List[str]\n",
    "    acts: List[float]\n",
    "\n",
    "    def __str__(self):\n",
    "        colored_tokens = [\n",
    "            f\"\\033[91m{token} (+{act:.2f})\\033[0m\" if act > 0 else token\n",
    "            for token, act in zip(self.token_strs, self.acts)\n",
    "        ]\n",
    "        return ''.join(colored_tokens)\n",
    "\n",
    "\n",
    "def get_latent_activations(model, model_alias, tokenizer, prompts, latents: Tuple[int, int]) -> List[List[PerTokenLatentActivations]]:\n",
    "\n",
    "    layers_to_cache = sorted(list(set([layer for layer, idx in latents])))\n",
    "\n",
    "    toks, activations = get_cache(model, tokenizer, prompts, cache_layers=layers_to_cache)\n",
    "\n",
    "    latents_activations = [[] for _ in range(len(latents))]\n",
    "\n",
    "    prompt_tok_ids = []\n",
    "    prompt_tok_strs = []\n",
    "\n",
    "    for batch_idx, prompt in enumerate(prompts):\n",
    "        n_toks = (toks[batch_idx] != tokenizer.pad_token_id).sum()\n",
    "        n_toks -= 1 # exclude the bos token\n",
    "        prompt_tok_ids.append(toks[batch_idx, -n_toks:].tolist())\n",
    "        prompt_tok_strs.append([tokenizer.decode(tok) for tok in prompt_tok_ids[-1]])\n",
    "\n",
    "    for layer_idx, layer in enumerate(layers_to_cache):\n",
    "        sae = load_sae_(model_alias, layer, repo_id=model_alias_to_sae_repo_id[model_alias])\n",
    "        latent_activations_at_layer = sae.encode(activations[:, layer_idx, :, :])\n",
    "\n",
    "        for batch_idx, prompt in enumerate(prompts):\n",
    "            n_toks = len(prompt_tok_ids[batch_idx])\n",
    "            for latent_idx_in_list, (layer_, latent_idx) in enumerate(latents):\n",
    "                if layer_ == layer:\n",
    "                    acts = latent_activations_at_layer[batch_idx, -n_toks:, latent_idx].tolist()\n",
    "                    latents_activations[latent_idx_in_list].append(\n",
    "                        PerTokenLatentActivations(prompt=prompt, token_ids=prompt_tok_ids[batch_idx], token_strs=prompt_tok_strs[batch_idx], acts=acts)\n",
    "                    )\n",
    "        \n",
    "    return latents_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(data, start_time: str, end_time: str, prompt_type=\"song_performers\", max_num_items=None):\n",
    "\n",
    "    start_time_year = int(start_time.split('-')[0])\n",
    "    start_time_month = int(start_time.split('-')[1])\n",
    "    end_time_year = int(end_time.split('-')[0])\n",
    "    end_time_month = int(end_time.split('-')[1])\n",
    "\n",
    "    def filter_by_time_range(element):\n",
    "        if start_time:\n",
    "            year = int(element['album_release_date'].split('-')[0])\n",
    "            month = int(element['album_release_date'].split('-')[1]) if len(element['album_release_date'].split('-')) > 1 else 1\n",
    "            if year < start_time_year or (year == start_time_year and month < start_time_month):\n",
    "                return False\n",
    "            if year > end_time_year or (year == end_time_year and month > end_time_month):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def preprocess_name(name: str):\n",
    "        # Remove text inside brackets\n",
    "        name = re.sub(r'[\\(\\[].*?[\\)\\]]', '', name)\n",
    "        name = name.split(\"-\")[0].strip()\n",
    "        return name\n",
    "\n",
    "    def create_prompt(element):\n",
    "        song_name = preprocess_name(element['name'])\n",
    "        if prompt_type == \"song_performers\":\n",
    "            return f\"The song '{song_name}'\"\n",
    "        else:\n",
    "            raise ValueError(f\"Prompt type {prompt_type} not supported\")\n",
    "\n",
    "    if prompt_type == \"song_performers\":\n",
    "        new_data = []\n",
    "        for element in data:\n",
    "            artist_songs = []\n",
    "            for song in element['tracks']:\n",
    "                if filter_by_time_range(song):\n",
    "                    artist_songs.append(song)\n",
    "            if max_num_items is not None and len(artist_songs) > max_num_items:\n",
    "                artist_songs = random.sample(artist_songs, max_num_items)\n",
    "            new_data.extend(artist_songs)\n",
    "        data = new_data\n",
    "    else:\n",
    "        raise ValueError(f\"Prompt type {prompt_type} not supported\")\n",
    "\n",
    "    if max_num_items is not None and len(data) > max_num_items:\n",
    "        data = random.sample(data, max_num_items)\n",
    "\n",
    "    prompts = [create_prompt(element) for element in data]\n",
    "\n",
    "    return prompts, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_by_model = {\n",
    "    'gemma-2-2b': [\n",
    "        (13, 7957), # known\n",
    "        (15, 11898), # unknown\n",
    "    ],\n",
    "    'gemma-2-9b': [\n",
    "        (22, 10424), # known\n",
    "        (21, 4451), # unknown\n",
    "    ],\n",
    "    'meta-llama_Llama-3.1-8B': [\n",
    "        (13, 21306), # known\n",
    "        (15, 28509), # unknown\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_songs_data_path = \"./raw_songs.json\"\n",
    "\n",
    "with open(cutoff_songs_data_path, \"r\") as f:\n",
    "    cutoff_songs_data = json.load(f)\n",
    "\n",
    "cutoff_songs_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_prompts, known_songs = create_prompts(cutoff_songs_data, start_time=\"2010-01\", end_time=\"2024-01\", max_num_items=500)\n",
    "unknown_prompts, unknown_songs = create_prompts(cutoff_songs_data, start_time=\"2024-08\", end_time=\"2025-01\", max_num_items=500)\n",
    "\n",
    "known_prompts = list(set(known_prompts))\n",
    "unknown_prompts = list(set(unknown_prompts))\n",
    "\n",
    "print(f\"len(known_prompts): {len(known_prompts)}\")\n",
    "print(f\"len(unknown_prompts): {len(unknown_prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import model_is_chat_model\n",
    "\n",
    "latents = latents_by_model[model_alias]\n",
    "\n",
    "known_prompts_latents_activations = defaultdict(list)\n",
    "unknown_prompts_latents_activations = defaultdict(list)\n",
    "prompts = unknown_prompts#[:10]\n",
    "if model_is_chat_model[model_alias] == True:\n",
    "    prompts = [format_instructions_chat_fn(instruction=p) for p in prompts]\n",
    "\n",
    "latents_activations = get_latent_activations(model, model_alias, tokenizer, known_prompts, latents)\n",
    "for latent_idx, latent in enumerate(latents):\n",
    "    known_prompts_latents_activations[latent].extend(latents_activations[latent_idx])\n",
    "\n",
    "\n",
    "latents_activations = get_latent_activations(model, model_alias, tokenizer, unknown_prompts, latents)\n",
    "for latent_idx, latent in enumerate(latents):\n",
    "    unknown_prompts_latents_activations[latent].extend(latents_activations[latent_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unknown_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_activation_frequency(latent_activations):\n",
    "    frequency = defaultdict(int)\n",
    "\n",
    "    for latent_idx, latent in enumerate(latent_activations):\n",
    "        print(latent_activations[latent])\n",
    "        for prompt_acts in latent_activations[latent]:\n",
    "            last_pos = len(prompt_acts.token_strs)git clone - 1\n",
    "            if prompt_acts.acts[last_pos] > 0 or prompt_acts.acts[last_pos-1] > 0:\n",
    "                frequency[latent] += 1\n",
    "        print('total', len(latent_activations[latent]))\n",
    "        frequency_percentage = frequency[latent] / len(latent_activations[latent])\n",
    "        print(f\"Latent L{latent[0]} F{latent[1]}: {frequency[latent]} ({frequency_percentage:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_activation_frequency(unknown_prompts_latents_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_prompts = [\n",
    "#     ('Who is the main actor in The Lord of the Rings?', 'Who is the main actor in The Man of the Rings?'),\n",
    "#     ('Who directed the movie 12 Angry Men?', 'Who directed the movie 200 Angry Men?'),\n",
    "#     ('What was the release year of the movie The Lord of the Rings?', 'What was the release year of the movie The Man of the Rings?'),\n",
    "#     ('Who is the director of the movie Pulp Fiction?', 'Who is the director of the movie Pulping Fiction?'),\n",
    "#     ('When was the player Cristiano Ronaldo born?', 'When was the player Cristiano Penalda born?'),\n",
    "#     ('Where was the player Leo Messi born?', 'How many goals did the player Leo Messi score in his career?'),\n",
    "#     (\"What team (name at least one) signed the player 'Jake Bornheimer'?\", \"What team (name at least one) signed the player 'Jeff Van Gundy'?\"),\n",
    "#     (\"What is the name of an actor starring in the movie 'The Ten Gladiators'?\", \"What is the name of an actor starring in the movie 'The Ten Gladiators'?\"),\n",
    "#     (\"What genre label best describes the movie 'Stranger by the Lake'?\", \"What genre label best describes the movie 'Stranger by the Lake'?\")\n",
    "# ]\n",
    "\n",
    "# question_prompts = [\n",
    "#     ('Who is Michael Jordan?', 'When was Michael Joordan born?'),\n",
    "#     ('When was Michael Jordan born?', 'How many points did Michael Jordan score in his career?'),\n",
    "#     ('In which year did Michael Jordan retire?', 'What team did Michael Jordan play for in his career?'),\n",
    "#     ('Who directed the movie the Godfather?', 'Who directed the movie the Godmother?'),\n",
    "#     ('What was the release year of the movie The Lord of the Rings?', 'What was the release year of the movie The Man of the Rings?'),\n",
    "#     ('Who is the director of the movie Pulp Fiction?', 'Who is the director of the movie Pulping Fiction?'),\n",
    "#     ('When was the player Cristiano Ronaldo born?', 'When was the player Cristiano Penalda born?'),\n",
    "# ]\n",
    "question_prompts = [\n",
    "    ('When was the player LeBron James born?', 'When was the player Wilson Brown born?'),\n",
    "    ('How many iPhones were sold in 2008?', 'What are the sizes of an iPhone 13?'),\n",
    "    ('What was the release year of the movie The Lord of the Rings?', 'What was the release year of the movie The Man of the Rings?'),\n",
    "    ('Who is the director of the movie Pulp Fiction?', 'Who is the director of the movie Pulping Fiction?'),\n",
    "    ('When was the player Cristiano Ronaldo born?', 'When was the player Cristiano Penalda born?'),\n",
    "]\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for m1, m2 in question_prompts:\n",
    "    prompts.append(m1)\n",
    "    prompts.append(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_contrastive_pairs = [\n",
    "    ('The Lord of the Rings', 'The Man of the Rings'),\n",
    "    ('12 Angry Men', '20 Angry Men'),\n",
    "    ('Do you know the movie The Lord of the Rings?', 'Do you know the movie The Man of the Rings?'),\n",
    "    ('Have you seen the movie Pulp Fiction?', 'Have you seen the movie Pulping Fiction?'),\n",
    "    (\"Let's go watch the movie 12 Angry Men\", \"Let's go watch the movie 20 Angry Men\"),\n",
    "    ('I watched the movie Good Will Hunting the other day.', 'I watched the movie Good William Hunting the other day.'),\n",
    "    ('The movie The Godfather is such a great movie.', 'The movie The Godmother is such a great movie.'),\n",
    "    \n",
    "]\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for m1, m2 in movies_contrastive_pairs:\n",
    "    prompts.append(m1)\n",
    "    prompts.append(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_contrastive_pairs = [\n",
    "    ('Paris', 'Parris'),\n",
    "    ('London', 'Londen'),\n",
    "    ('Have you been to Paris?', 'Have you been to Parris?'),\n",
    "    ('I am visiting Barcelona next week.', 'I am visiting Sarbelona next week.'),\n",
    "    ('I was born in New York City.', 'I was born in Old Dork City.'),\n",
    "    ('Do you know the city of San Francisco?', 'Do you know the city of Can Sancisco?'),\n",
    "]\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for c1, c2 in city_contrastive_pairs:\n",
    "    prompts.append(c1)\n",
    "    prompts.append(c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_alterations = [\n",
    "    (\"The Godfather\", \"The Dogfather\"),\n",
    "    (\"Jurassic Park\", \"Jurassic Parking\"),\n",
    "    (\"The Silence of the Lambs\", \"The Silence of the Hams\"),\n",
    "    (\"Forrest Gump\", \"Forrest Plump\"),\n",
    "    (\"The Matrix\", \"The Mattress\"),\n",
    "    (\"Schindler's List\", \"Schindler's Lisp\"),\n",
    "    (\"The Shawshank Redemption\", \"The Shawshank Seduction\"),\n",
    "    (\"Titanic\", \"Gigantic\"),\n",
    "    (\"The Dark Knight\", \"The Dork Knight\"),\n",
    "    (\"Eternal Sunshine of the Spotless Mind\", \"Eternal Sunshine of the Spotless Behind\")\n",
    "]\n",
    "\n",
    "movie_phrases = [\n",
    "    (\"The Godfather\", \"The Dogfather\"),\n",
    "    (\"The movie Jurassic Park was released in 1993.\", \"The movie Jurassic Parking was released in 1993.\"),\n",
    "    (\"Do you know the movie The Silence of the Lambs?\", \"Do you know the movie The Silence of the Hams?\"),\n",
    "    (\"Have you seen the movie Forrest Gump?\", \"Have you seen the movie Forrest Plump?\"),\n",
    "    (\"Let's go watch the movie The Matrix\", \"Let's go watch the movie The Mattress\"),\n",
    "    (\"I watched the movie Schindler's List last night.\", \"I watched the movie Schindler's Lisp last night.\"),\n",
    "    (\"The movie The Shawshank Redemption is a classic.\", \"The movie The Shawshank Seduction is a classic.\"),\n",
    "    (\"Titanic\", \"Gigantic\"),\n",
    "    (\"My favorite superhero film is The Dark Knight.\", \"My favorite superhero film is The Dork Knight.\"),\n",
    "    (\"Eternal Sunshine of the Spotless Mind is a unique love story.\", \"Eternal Sunshine of the Spotless Behind is a unique love story.\")\n",
    "]\n",
    "\n",
    "city_alterations = [\n",
    "    (\"New York\", \"New Pork\"),\n",
    "    (\"Los Angeles\", \"Lost Angeles\"),\n",
    "    (\"I love visiting Paris\", \"I love visiting Pairs\"),\n",
    "    (\"Have you ever been to London?\", \"Have you ever been to Londonut?\"),\n",
    "    (\"Tokyo is a bustling metropolis\", \"Tokyolk is a bustling metropolis\"),\n",
    "    (\"Rome\", \"Roam\"),\n",
    "    (\"Let's take a trip to Sydney\", \"Let's take a trip to Kidney\"),\n",
    "    (\"Berlin is known for its history\", \"Berlout is known for its history\"),\n",
    "    (\"Moscow\", \"Cowscow\"),\n",
    "    (\"The weather in Chicago is unpredictable\", \"The weather in Chicagoing is unpredictable\")\n",
    "]\n",
    "\n",
    "athlete_alterations = [\n",
    "    (\"LeBron James\", \"LeBroom Games\"),\n",
    "    (\"Serena Williams\", \"Serenade Williams\"),\n",
    "    (\"Did you see Cristiano Ronaldo's goal?\", \"Did you see Cristina Penalda's goal?\"),\n",
    "    (\"Usain Bolt is the fastest man alive\", \"Usain Jolt is the fastest man alive\"),\n",
    "    (\"Michael Phelps\", \"Michael Yelps\"),\n",
    "    (\"I admire the skills of Simone Biles\", \"I admire the skills of Lemon Biles\"),\n",
    "    (\"Roger Federer\", \"Roger Cheddar\"),\n",
    "    (\"Megan Rapinoe led the team to victory\", \"Vegan Rapinoe led the team to victory\"),\n",
    "    (\"Tom Brady\", \"Tom Gravy\"),\n",
    "    (\"The legacy of Muhammad Ali is inspiring\", \"The legacy of Muhammad Alley is inspiring\")\n",
    "]\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for m1, m2 in movie_phrases + city_alterations + athlete_alterations:\n",
    "    prompts.append(m1)\n",
    "    prompts.append(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('./random_entities/unknown_entities_sentences.json', 'r') as f:\n",
    "    known_entities_data = json.load(f)\n",
    "\n",
    "# Extract sentences and entity names\n",
    "known_entity_sentences = [item['sentence'] for item in known_entities_data]\n",
    "known_entity_names = [item['entity_name'] for item in known_entities_data]\n",
    "\n",
    "print(f\"Loaded {len(known_entity_sentences)} known entity sentences.\")\n",
    "print(\"First 5 sentences:\")\n",
    "for sentence in known_entity_sentences[:5]:\n",
    "    print(f\"- {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompts = known_entity_sentences[:20]\n",
    "\n",
    "random_ids = random.sample(range(len(known_entity_sentences)), 20)\n",
    "prompts = [known_entity_sentences[i] for i in random_ids]\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import model_is_chat_model\n",
    "\n",
    "latents = [\n",
    "    #(13, 3130)\n",
    "    # (13, 3130)\n",
    "    #(13, 7957), # known\n",
    "    #(15, 11898), # unknown\n",
    "    # (14, 7769), # unknown\n",
    "    # (13, 6), # unknown\n",
    "    # (7, 3782), # unknown\n",
    "    # (11, 8468), # unknown\n",
    "    (14, 25742)\n",
    "]\n",
    "\n",
    "if model_is_chat_model[model_alias] == True:\n",
    "    prompts = [format_instructions_chat_fn(instruction=p) for p in prompts]\n",
    "latents_activations = get_latent_activations(model, model_alias, tokenizer, prompts, latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = latents[0]\n",
    "idx = latents.index(latent)\n",
    "\n",
    "print(f\"Latent L{latent[0]} F{latent[1]}:\")\n",
    "for prompt_acts in latents_activations[idx]:\n",
    "    print(prompt_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = latents[0]\n",
    "idx = latents.index(latent)\n",
    "\n",
    "print(f\"Latent L{latent[0]} F{latent[1]}:\")\n",
    "for prompt_acts in latents_activations[idx]:\n",
    "    print(prompt_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = latents[0]\n",
    "idx = latents.index(latent)\n",
    "\n",
    "print(f\"Latent L{latent[0]} F{latent[1]}:\")\n",
    "for prompt_acts in latents_activations[idx]:\n",
    "    print(prompt_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_contrastive_prompts = [\n",
    "    (\"Michael Jordan scored 30 points last night.\", \"Michael Joordan scored 30 points last night.\"),\n",
    "    (\"The movie Jurassic Park was released in 1993.\", \"The movie Jurassic Parking was released in 1993.\"),\n",
    "    (\"I've watched 12 Angry Men many times already.\", \"I've watched 20 Angry Men many times already.\"),\n",
    "    (\"The city of Berlin is known for its history.\", \"The city of Berlouin is known for its history.\"),\n",
    "    (\"Michael Phelps just won his 23rd Olympic gold medal\", \"Michael Yelps just won his 23rd Olympic gold medal\"),\n",
    "    (\"The legacy of Muhammad Ali is inspiring\", \"The legacy of Muhammad Alley is inspiring\"),\n",
    "]\n",
    "\n",
    "prompts = []\n",
    "for m1, m2 in final_contrastive_prompts:\n",
    "    prompts.append(m1)\n",
    "    prompts.append(m2)\n",
    "\n",
    "latents_activations = get_latent_activations(model, model_alias,tokenizer, prompts, latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = (15, 11898)\n",
    "idx = latents.index(latent)\n",
    "\n",
    "print(f\"Latent L{latent[0]} F{latent[1]}:\")\n",
    "for prompt_acts in latents_activations[idx]:\n",
    "    print(prompt_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt_acts in latents_activations[idx]:\n",
    "    print(\"[\", end=\"\")\n",
    "    for token_str, token_act in zip(prompt_acts.token_strs, prompt_acts.acts):\n",
    "        print(f\"({repr(token_str)}, {token_act:.2f})\", end=\" \")\n",
    "    print(\"]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
