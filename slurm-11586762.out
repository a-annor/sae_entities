Loading rhel8/default-amp
  Loading requirement: rhel8/slurm singularity/current rhel8/global cuda/11.4
    libpciaccess/0.16/gcc-9.4.0-6fonbj6 libiconv/1.16/gcc-9.4.0-ahebbov
    libxml2/2.9.12/gcc-9.4.0-gnknt5e ncurses/6.2/gcc-9.4.0-aiirok7
    hwloc/2.5.0/gcc-9.4.0-7sqomga libevent/2.1.12/gcc-9.4.0-hgny7cm
    numactl/2.0.14/gcc-9.4.0-52dwc6n cuda/11.4.0/gcc-9.4.0-3hnxhjt
    gdrcopy/2.2/gcc-9.4.0-e4igtfp knem/1.1.4/gcc-9.4.0-bpbxgva
    libnl/3.3.0/gcc-9.4.0-whwhrwb rdma-core/34.0/gcc-9.4.0-5eo5n2u
    ucx/1.11.1/gcc-9.4.0-lktqyl4 openmpi/4.1.1/gcc-9.4.0-epagguv
/var/spool/slurm/slurmd/job11586762/slurm_script: line 65: conda: command not found

An error occurred during the pipeline execution: [Errno 2] No such file or directory: 'data/{CATERGORY}/prompts/batch_1.jsonl'
----- Splitting data/Race_ethnicity/input/Race_ethnicity.jsonl into batches of 5000 lines -----
Changed directory to /rds/user/ana42/hpc-work/sae_entities.

JobID: 11586762
======
Time: Sat Jun 28 20:42:25 BST 2025
Running on master node: gpu-q-1
Current directory: /rds/user/ana42/hpc-work/sae_entities

Nodes allocated:
================
gpu-q-1

numtasks=1, numnodes=1, mpi_tasks_per_node=1 (OMP_NUM_THREADS=1)

Executing command:
==================
python -u utils_bias/process_data.py  > logs/out.11586762


An error occurred during the pipeline execution: [Errno 2] No such file or directory: 'data/{CATERGORY}/prompts/batch_1.jsonl'
